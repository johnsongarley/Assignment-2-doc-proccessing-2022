{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9pWsPAt0dCh"
   },
   "source": [
    "# Assignment 2 - Find complex answers to medical questions\n",
    "\n",
    "**Submission deadline: Friday 22 April, 5pm.** \n",
    "\n",
    "Late submissions **will not be accepted** without an approved [Special Consideration](http://from.mq.edu.au/MT0X0E0FUrrU200rm0JB0U0) request.  Assessments submitted after the due date will receive a mark of **zero**.\n",
    "\n",
    "**Assessment marks: 20 marks (20% of the total unit assessment)**\n",
    "\n",
    "In this assignment we will work on a task of \"query-focused summarisation\" on medical questions where the goal is, given a medical question and a list of sentences extracted from relevant medical publications, to determine which of these sentences from the list can be used as part of the answer to the question.\n",
    "\n",
    "We will use data that has been derived from the **BioASQ challenge** (http://www.bioasq.org/), after some data manipulation to make it easier to process for this assignment. The BioASQ challenge organises several \"shared tasks\", including a task on biomedical semantic question answering which we are using here. The data are in the file `bioasq10_labelled.csv`, which is part of the zip file provided. Each row of the file has a question, a sentence text, and a label that indicates whether the sentence text is part of the answer to the question (1) or not (0).\n",
    "\n",
    "The following code uses pandas to store the file `bioasq10_labelled.csv` in a data frame and show the first rows of data. For this code to run, first you need to unzip the file `data.zip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>sentid</th>\n",
       "      <th>question</th>\n",
       "      <th>sentence text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Hirschsprung disease (HSCR) is a multifactoria...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>In this study, we review the identification of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>The majority of the identified genes are relat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>The non-Mendelian inheritance of sporadic non-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Coding sequence mutations in e.g.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qid  sentid                                           question  \\\n",
       "0    0       0  Is Hirschsprung disease a mendelian or a multi...   \n",
       "1    0       1  Is Hirschsprung disease a mendelian or a multi...   \n",
       "2    0       2  Is Hirschsprung disease a mendelian or a multi...   \n",
       "3    0       3  Is Hirschsprung disease a mendelian or a multi...   \n",
       "4    0       4  Is Hirschsprung disease a mendelian or a multi...   \n",
       "\n",
       "                                       sentence text  label  \n",
       "0  Hirschsprung disease (HSCR) is a multifactoria...      0  \n",
       "1  In this study, we review the identification of...      1  \n",
       "2  The majority of the identified genes are relat...      1  \n",
       "3  The non-Mendelian inheritance of sporadic non-...      1  \n",
       "4                  Coding sequence mutations in e.g.      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"bioasq10b_labelled.csv\")\n",
    "dataset[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the CSV file are:\n",
    "\n",
    "* `qid`: an ID for a question. Several rows may have the same question ID, as we can see above.\n",
    "* `sentid`: an ID for a sentence.\n",
    "* `question`: The text of the question. In the above example, the first rows all have the same question: \"Is Hirschsprung disease a mendelian or a multifactorial disorder?\"\n",
    "* `sentence text`: The text of the sentence.\n",
    "* `label`: 1 if the sentence is a part of the answer, 0 if the sentence is not part of the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6xqxCmR0dCk"
   },
   "source": [
    "# Task 1 (5 marks): Data preparation\n",
    "\n",
    "Partition the data into the training, dev_test, and test sets using the proportions 6:2:2. That is, 60% of the questions must be in the training set, 20% must be in the dev_test set, and the remaining 20% in the test set. Make sure that you partition based on the questions, not on the rows. With this we mean that all the sentences related to a question must be in one file only. In other words, there must not be sentences from the same question in, say, the training and the test data.\n",
    "\n",
    "Also, make sure that you implement a random partition.\n",
    "\n",
    "Save the partitions as the files `training.csv`, `dev_test.csv`, and `test.csv`, so that they can be used by other people.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if your explanation answers the following question correctly: Why do we want to split the partition on the questions, and not on the rows?\n",
    "* **1 mark** if the code partitions the data on the questions randomly and according to the split 6:2:2.\n",
    "* **1 mark** if your code generates partitions that have similar balance of labels and you demonstrate that they are similar.\n",
    "* **1 mark** if the partitions are saved as the CSV files `training.csv`, `dev_test.csv`, and `test.csv`.\n",
    "* **1 mark** for good coding and documentation in this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fhqUtqJL0dCm"
   },
   "outputs": [],
   "source": [
    "#random shuffling of each group of question\n",
    "import random\n",
    "random.seed(1234)\n",
    "groups = [dataset for i , dataset in dataset.groupby('qid')]\n",
    "random.shuffle(groups)\n",
    "dataset2 = pd.concat(groups).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fcfb1812190>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multiindexing the entire dataset so it easy to split\n",
    "dataset2.set_index(['qid', 'sentid'], inplace=True)\n",
    "dataset2.groupby(level=[0,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4234"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the count of unique questions\n",
    "maxi = dataset2.index.get_level_values('qid').nunique()\n",
    "maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training table\n",
    "rowcount = 0\n",
    "store = 0\n",
    "for result in dataset2.index.get_level_values('qid').unique():\n",
    "    if store != int(maxi*0.6):\n",
    "        rowcount += dataset2.loc[result,\"question\"].count()\n",
    "        store+=1\n",
    "training_df = pd.DataFrame(dataset2[0:rowcount])\n",
    "training_df = training_df.reset_index()\n",
    "training_df.to_csv('training.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>sentid</th>\n",
       "      <th>question</th>\n",
       "      <th>sentence text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3143</td>\n",
       "      <td>0</td>\n",
       "      <td>Velocardial facial syndrome, otherwise known a...</td>\n",
       "      <td>The deletion of chromosome 22q11.2 is involved...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3143</td>\n",
       "      <td>1</td>\n",
       "      <td>Velocardial facial syndrome, otherwise known a...</td>\n",
       "      <td>deletions of chromosome 7q11.23 (Williams synd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3143</td>\n",
       "      <td>2</td>\n",
       "      <td>Velocardial facial syndrome, otherwise known a...</td>\n",
       "      <td>Submicroscopic deletions of chromosome 22q11 h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3143</td>\n",
       "      <td>3</td>\n",
       "      <td>Velocardial facial syndrome, otherwise known a...</td>\n",
       "      <td>The 22q11.2 deletion syndrome (di George syndr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3143</td>\n",
       "      <td>4</td>\n",
       "      <td>Velocardial facial syndrome, otherwise known a...</td>\n",
       "      <td>UNLABELLED\\nMost of the children with Di Georg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid  sentid                                           question  \\\n",
       "0  3143       0  Velocardial facial syndrome, otherwise known a...   \n",
       "1  3143       1  Velocardial facial syndrome, otherwise known a...   \n",
       "2  3143       2  Velocardial facial syndrome, otherwise known a...   \n",
       "3  3143       3  Velocardial facial syndrome, otherwise known a...   \n",
       "4  3143       4  Velocardial facial syndrome, otherwise known a...   \n",
       "\n",
       "                                       sentence text  label  \n",
       "0  The deletion of chromosome 22q11.2 is involved...      1  \n",
       "1  deletions of chromosome 7q11.23 (Williams synd...      0  \n",
       "2  Submicroscopic deletions of chromosome 22q11 h...      1  \n",
       "3  The 22q11.2 deletion syndrome (di George syndr...      1  \n",
       "4  UNLABELLED\\nMost of the children with Di Georg...      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"training.csv\")[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_test table\n",
    "dataset3 = pd.DataFrame(dataset2[rowcount:len(dataset2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi2 = dataset3.index.get_level_values('qid').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowcount2 = 0\n",
    "store2 = 0\n",
    "for result in dataset3.index.get_level_values('qid').unique():\n",
    "    if store2 != int(maxi2*0.5):\n",
    "        rowcount2 += dataset3.loc[result,\"question\"].count()\n",
    "        store2+=1\n",
    "dev_test_df = pd.DataFrame(dataset3[0:rowcount2])\n",
    "dev_test_df = dev_test_df.reset_index()\n",
    "dev_test_df.to_csv('dev_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>sentid</th>\n",
       "      <th>question</th>\n",
       "      <th>sentence text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2307</td>\n",
       "      <td>0</td>\n",
       "      <td>Do cephalopods use RNA editing less frequently...</td>\n",
       "      <td>Extensive messenger RNA editing generates tran...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2307</td>\n",
       "      <td>1</td>\n",
       "      <td>Do cephalopods use RNA editing less frequently...</td>\n",
       "      <td>By adopting a method originally designed to de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2307</td>\n",
       "      <td>2</td>\n",
       "      <td>Do cephalopods use RNA editing less frequently...</td>\n",
       "      <td>We here show that RNA editing is particularly ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2307</td>\n",
       "      <td>3</td>\n",
       "      <td>Do cephalopods use RNA editing less frequently...</td>\n",
       "      <td>Even for the subset of RNA editing sites share...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2307</td>\n",
       "      <td>4</td>\n",
       "      <td>Do cephalopods use RNA editing less frequently...</td>\n",
       "      <td>Coleoid cephalopods (octopus, squid and cuttle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid  sentid                                           question  \\\n",
       "0  2307       0  Do cephalopods use RNA editing less frequently...   \n",
       "1  2307       1  Do cephalopods use RNA editing less frequently...   \n",
       "2  2307       2  Do cephalopods use RNA editing less frequently...   \n",
       "3  2307       3  Do cephalopods use RNA editing less frequently...   \n",
       "4  2307       4  Do cephalopods use RNA editing less frequently...   \n",
       "\n",
       "                                       sentence text  label  \n",
       "0  Extensive messenger RNA editing generates tran...      1  \n",
       "1  By adopting a method originally designed to de...      1  \n",
       "2  We here show that RNA editing is particularly ...      0  \n",
       "3  Even for the subset of RNA editing sites share...      1  \n",
       "4  Coleoid cephalopods (octopus, squid and cuttle...      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('dev_test.csv')[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataset\n",
    "test_df = pd.DataFrame(dataset3[rowcount2:len(dataset3)])\n",
    "test_df = test_df.reset_index()\n",
    "test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>sentid</th>\n",
       "      <th>question</th>\n",
       "      <th>sentence text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3633</td>\n",
       "      <td>0</td>\n",
       "      <td>Which method has been developed for mapping of...</td>\n",
       "      <td>SLIC-CAGE: high-resolution transcription start...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3633</td>\n",
       "      <td>1</td>\n",
       "      <td>Which method has been developed for mapping of...</td>\n",
       "      <td>Cap analysis of gene expression (CAGE) is a me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3633</td>\n",
       "      <td>2</td>\n",
       "      <td>Which method has been developed for mapping of...</td>\n",
       "      <td>In combination with high-throughput sequencing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3633</td>\n",
       "      <td>3</td>\n",
       "      <td>Which method has been developed for mapping of...</td>\n",
       "      <td>The biggest limitation of CAGE is that even th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3633</td>\n",
       "      <td>4</td>\n",
       "      <td>Which method has been developed for mapping of...</td>\n",
       "      <td>Here, we present SLIC-CAGE, a Super-Low Input ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid  sentid                                           question  \\\n",
       "0  3633       0  Which method has been developed for mapping of...   \n",
       "1  3633       1  Which method has been developed for mapping of...   \n",
       "2  3633       2  Which method has been developed for mapping of...   \n",
       "3  3633       3  Which method has been developed for mapping of...   \n",
       "4  3633       4  Which method has been developed for mapping of...   \n",
       "\n",
       "                                       sentence text  label  \n",
       "0  SLIC-CAGE: high-resolution transcription start...      0  \n",
       "1  Cap analysis of gene expression (CAGE) is a me...      1  \n",
       "2  In combination with high-throughput sequencing...      1  \n",
       "3  The biggest limitation of CAGE is that even th...      0  \n",
       "4  Here, we present SLIC-CAGE, a Super-Low Input ...      1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"test.csv\")[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbUJWlD_0dCv"
   },
   "source": [
    "# Task 2 (5 marks): Cosine similarity\n",
    "\n",
    "Use the files `training.csv`, `dev_test.csv`, and `test.csv` provided by us in the file `data.zip` (so that any possible errors that you may have introduced in task 1 do not propagate to this task and following tasks).\n",
    "\n",
    "Implement a simple text summariser that is based on the cosine similarity between the question and the text. Use the following function signature.\n",
    "\n",
    "```{python}\n",
    "def cosine_summariser(csvfile, questionids, n=5):\n",
    "   \"\"\"Return the IDs of the n sentences that have the highest cosine similarity\n",
    "    with the question. The input questionids is a list of question ids. The \n",
    "    output is a list of lists of sentence ids\n",
    "    >>> cosine_summariser('test.csv', [3, 11], 3)\n",
    "    [[3, 1, 4], [12, 4, 13]]\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "To obtain the text vectors, use sklearn's tf.idf libraries this way:\n",
    "\n",
    "* Use all the defaults from the TfidfVectorizer instance, except for `stop_words=\"english\"` and `max_features=10000`. The latter option will restrict the vocabulary size to 10,000. This will speed up the computations and reduce the memory footprint in the subsequent tasks.\n",
    "* Use the `fit` method on the text of `training.csv`. In your documentation, please explain and justify what decision choices you made to select the correct text: would you use the question text only, the sentence text, or both?\n",
    "\n",
    "Evaluate the summariser by reporting the mean F1 score on each of the three CSV files `training.csv`, `devtest.csv`, and `test.csv`, for $n=5$. To calculate the mean F1 score, do this:\n",
    "\n",
    "1. For each question ID in the file, calculate the F1 score by comparing the result of your cosine summariser and the given labels. Feel free to use sklearn's functions to compute the F1 score, or implement your own version of the F1 scoring function if you prefer.\n",
    "2. Calculate the mean of the F1 scores calculated in step 1.\n",
    "\n",
    "Find the value of $n$ that returns the highest mean F1 score on the dev_test data.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the code generates the tf.idf vectors correctly. The explanations that justify the decisions made are reasonable. In particular, explain and justify what information you used to fit tf.idf.\n",
    "* **1 mark** if the code calculates cosine similarity correctly.\n",
    "* **1 mark** if the code returns the IDs of the $n$ sentences that have the highest cosine similarity with the question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the dev_test file and identifies the value of $n$ that gives the highest score on the dev_test file.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, comment on the reason why you think the value of $n$ that gives highest F1 has that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "UAr6x5Zc0dCw",
    "outputId": "7fa58ee8-d420-47ce-e925-c37950857924"
   },
   "outputs": [],
   "source": [
    "# we will use the sentences only to conduct the tfidf. \n",
    "#it is the respresentation of the text. \n",
    "#the question has no relation to the text. so it can only be used as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conducting the tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_Read():\n",
    "    df = pd.read_csv(\"data/training.csv\")\n",
    "    corpus = list()\n",
    "    counter = 0\n",
    "    for i in df['qid'].unique():\n",
    "        new_table = df.loc[df['qid'] == i]\n",
    "        string = ''\n",
    "        for j in range(len(new_table)):\n",
    "            if counter != len(new_table):\n",
    "                string += new_table['sentence text'].loc[counter] + ' '\n",
    "                counter += 1\n",
    "        corpus.append(string)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_summariser(csvfile, questionids, n):\n",
    "    df = pd.read_csv(csvfile)\n",
    "    corpus = cosine_Read()\n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\", use_idf = True, max_features=10000)\n",
    "    matrix = tfidf.fit(corpus)\n",
    "    result = dict()\n",
    "    main_result = list()\n",
    "    for i in questionids:\n",
    "        counter = 0\n",
    "        new_table = df.loc[df['qid'] == i]\n",
    "        for j in range(len(new_table)):\n",
    "            if counter != len(new_table):\n",
    "                tfidf_norm_values = matrix.transform([new_table['sentence text'].iloc[counter]]).toarray()\n",
    "                tfidf_norm_values2 = matrix.transform([new_table['question'].iloc[counter]]).toarray()\n",
    "                temp = pairwise.cosine_similarity(tfidf_norm_values,tfidf_norm_values2)\n",
    "                result[counter] = temp[0][0]\n",
    "            counter += 1\n",
    "        sorted_result = sorted(result.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        store = [x for x, r in sorted_result]\n",
    "        main_result.append(store[0:n])\n",
    "        store = []\n",
    "    return main_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 6, 9, 11, 1], [8, 16, 5, 7, 18]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_summariser(\"training.csv\", [3143, 20], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VTTgRnN0dC4"
   },
   "source": [
    "# Task 3 (5 marks): Simple NN\n",
    "\n",
    "Use the files `training.csv`, `dev_test.csv`, and `test.csv` provided by us.\n",
    "\n",
    "Implement a simple TensorFlow-Keras neural model that has the following sequence of layers:\n",
    "\n",
    "1. An input layer that will accept the tf.idf of the sentence text (we will ignore the question text in this task). Use the TfidfVectorizer instance that you have fitted in task 2.\n",
    "2. A hidden layer and a relu activation function. You need to determine the size of the hidden layer.\n",
    "3. An output layer with one cell. The output layer will classify the input text (binary classification).\n",
    "\n",
    "Train the model with the training data and use the dev_test set to determine a good size of the hidden layer. \n",
    "\n",
    "With the model that you have trained, and implement a summariser that returns the $n$ sentences with highest predicted score. Use the following function signature:\n",
    "\n",
    "```{python}\n",
    "def nn_summariser(csvfile, questionids, n=5):\n",
    "   \"\"\"Return the IDs of the n sentences that have the highest predicted score. The input questionids is a list of question ids. The \n",
    "    output is a list of lists of sentence ids\n",
    "    >>> cosine_summariser('test.csv', [3, 11], 3)\n",
    "    [[2, 1, 3], [7, 14, 10]]\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "Report the final results using the test set. Remember: use the test set to report the final results of the best system only.\n",
    "\n",
    "Based on your experiments, comment on whether this system is better than the system developed in task 2. To make this task less time-consuming, focus only on $n=5$.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
    "* **1 mark** if the code passes the tf.idf information of the text to the model correctly.\n",
    "* **1 mark** if the code returns the IDs of the $n$ sentences that have the highest prediction score in the given question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the hidden layer. The explanations must be clear and concise. To make this task less time-consuming, use $n=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uY6sDbUn0dC6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>sentid</th>\n",
       "      <th>question</th>\n",
       "      <th>sentence text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>List signaling molecules (ligands) that intera...</td>\n",
       "      <td>the epidermal growth factor receptor (EGFR) li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>List signaling molecules (ligands) that intera...</td>\n",
       "      <td>EGFR ligands epidermal growth factor (EGF), a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>List signaling molecules (ligands) that intera...</td>\n",
       "      <td>EGFR and its ligand EGF</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>List signaling molecules (ligands) that intera...</td>\n",
       "      <td>Among EGFR ligands, heparin-binding EGF-like g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>List signaling molecules (ligands) that intera...</td>\n",
       "      <td>Plasma amphiregulin (AR), epidermal growth fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qid  sentid                                           question  \\\n",
       "0    1       0  List signaling molecules (ligands) that intera...   \n",
       "1    1       1  List signaling molecules (ligands) that intera...   \n",
       "2    1       2  List signaling molecules (ligands) that intera...   \n",
       "3    1       3  List signaling molecules (ligands) that intera...   \n",
       "4    1       4  List signaling molecules (ligands) that intera...   \n",
       "\n",
       "                                       sentence text  label  \n",
       "0  the epidermal growth factor receptor (EGFR) li...      1  \n",
       "1   EGFR ligands epidermal growth factor (EGF), a...      1  \n",
       "2                            EGFR and its ligand EGF      0  \n",
       "3  Among EGFR ligands, heparin-binding EGF-like g...      0  \n",
       "4   Plasma amphiregulin (AR), epidermal growth fa...      0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"data/training.csv\")\n",
    "dataset[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the dataset(training dataset)\n",
    "train = pd.read_csv(\"data/training.csv\")\n",
    "train_text = list(train['sentence text'])\n",
    "train_label = train.drop(labels = [\"qid\", \"sentid\", \"question\", \"sentence text\"], axis = 1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_test dataset preparation\n",
    "dev_test = pd.read_csv(\"data/dev_test.csv\")\n",
    "dev_test_text = list(dev_test['sentence text'])\n",
    "dev_test_label = dev_test.drop(labels = [\"qid\", \"sentid\", \"question\", \"sentence text\"], axis = 1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_test dataset preparation\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "test_text = list(test['sentence text'])\n",
    "test_label = test.drop(labels = [\"qid\", \"sentid\", \"question\", \"sentence text\"], axis = 1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(input ='content', stop_words=\"english\", max_features=10000)\n",
    "train_tfidf = tfidf.fit_transform(train_text).toarray()\n",
    "dev_test_tfidf = tfidf.fit_transform(dev_test_text).toarray()\n",
    "test_tfidf = tfidf.fit_transform(test_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37951, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               5120512   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 5,121,025\n",
      "Trainable params: 5,121,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 23:48:51.682836: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-21 23:48:51.683306: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation = \"relu\", input_shape = (train_tfidf.shape[1],)))\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37951 samples, validate on 12741 samples\n",
      "Epoch 1/20\n",
      "37951/37951 [==============================] - 8s 224us/sample - loss: 0.6052 - accuracy: 0.6958 - val_loss: 0.6228 - val_accuracy: 0.6972\n",
      "Epoch 2/20\n",
      "37951/37951 [==============================] - 8s 210us/sample - loss: 0.5390 - accuracy: 0.7236 - val_loss: 0.6469 - val_accuracy: 0.6802\n",
      "Epoch 3/20\n",
      "37951/37951 [==============================] - 8s 208us/sample - loss: 0.4870 - accuracy: 0.7695 - val_loss: 0.7049 - val_accuracy: 0.6739\n",
      "Epoch 4/20\n",
      "37951/37951 [==============================] - 8s 217us/sample - loss: 0.4461 - accuracy: 0.7965 - val_loss: 0.7633 - val_accuracy: 0.6592\n",
      "Epoch 5/20\n",
      "37951/37951 [==============================] - 8s 222us/sample - loss: 0.4127 - accuracy: 0.8170 - val_loss: 0.8503 - val_accuracy: 0.6601\n",
      "Epoch 6/20\n",
      "37951/37951 [==============================] - 9s 226us/sample - loss: 0.3838 - accuracy: 0.8340 - val_loss: 0.9420 - val_accuracy: 0.6584\n",
      "Epoch 7/20\n",
      "37951/37951 [==============================] - 9s 230us/sample - loss: 0.3568 - accuracy: 0.8488 - val_loss: 1.0232 - val_accuracy: 0.6516\n",
      "Epoch 8/20\n",
      "37951/37951 [==============================] - 9s 237us/sample - loss: 0.3319 - accuracy: 0.8624 - val_loss: 1.1232 - val_accuracy: 0.6540\n",
      "Epoch 9/20\n",
      "37951/37951 [==============================] - 9s 238us/sample - loss: 0.3079 - accuracy: 0.8757 - val_loss: 1.2124 - val_accuracy: 0.6479\n",
      "Epoch 10/20\n",
      "37951/37951 [==============================] - 9s 242us/sample - loss: 0.2852 - accuracy: 0.8880 - val_loss: 1.3090 - val_accuracy: 0.6456\n",
      "Epoch 11/20\n",
      "37951/37951 [==============================] - 9s 242us/sample - loss: 0.2631 - accuracy: 0.9005 - val_loss: 1.3888 - val_accuracy: 0.6389\n",
      "Epoch 12/20\n",
      "37951/37951 [==============================] - 9s 249us/sample - loss: 0.2417 - accuracy: 0.9120 - val_loss: 1.5173 - val_accuracy: 0.6433\n",
      "Epoch 13/20\n",
      "37951/37951 [==============================] - 10s 270us/sample - loss: 0.2214 - accuracy: 0.9231 - val_loss: 1.6117 - val_accuracy: 0.6399\n",
      "Epoch 14/20\n",
      "37951/37951 [==============================] - 9s 248us/sample - loss: 0.2018 - accuracy: 0.9325 - val_loss: 1.7080 - val_accuracy: 0.6387\n",
      "Epoch 15/20\n",
      "37951/37951 [==============================] - 10s 260us/sample - loss: 0.1828 - accuracy: 0.9414 - val_loss: 1.8588 - val_accuracy: 0.6420\n",
      "Epoch 16/20\n",
      "37951/37951 [==============================] - 10s 254us/sample - loss: 0.1654 - accuracy: 0.9492 - val_loss: 1.9575 - val_accuracy: 0.6383\n",
      "Epoch 17/20\n",
      "37951/37951 [==============================] - 10s 256us/sample - loss: 0.1486 - accuracy: 0.9550 - val_loss: 2.0983 - val_accuracy: 0.6412\n",
      "Epoch 18/20\n",
      "37951/37951 [==============================] - 10s 256us/sample - loss: 0.1339 - accuracy: 0.9605 - val_loss: 2.1939 - val_accuracy: 0.6370\n",
      "Epoch 19/20\n",
      "37951/37951 [==============================] - 10s 253us/sample - loss: 0.1200 - accuracy: 0.9647 - val_loss: 2.3644 - val_accuracy: 0.6405\n",
      "Epoch 20/20\n",
      "37951/37951 [==============================] - 10s 259us/sample - loss: 0.1079 - accuracy: 0.9680 - val_loss: 2.4601 - val_accuracy: 0.6395\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "history = model.fit(train_tfidf, np.array(train_label), \n",
    "                    epochs = 20, \n",
    "                    batch_size = 1000, \n",
    "                    validation_data = (dev_test_tfidf, np.array(dev_test_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAt/ElEQVR4nO3deZxU1Zn/8c/DTrMKsigNNBoU4Yc0bQcV0UA0Bpdo3EYIv0QwimiMMZkxmjGJjgnzykR/0TgxcTBRE8WgE5UYg3tco4k0igoIithoiwugArJDP78/zq3u6qKqu+iuW9XL9/163VfdvZ6+NPfpc84955q7IyIikqpdoQMQEZHmSQlCRETSUoIQEZG0lCBERCQtJQgREUlLCUJERNJSgpC8MrOHzOycXO9bSGZWaWbHxXBeN7PPRfM3m9mPstm3Ed8zzcwebWyc0nqZ+kFIQ8zss6TFImA7sDtavsDd5+Y/qubDzCqB89z98Ryf14Hh7r4yV/uaWQnwNtDR3XflJFBptToUOgBp/ty9e2K+vpuhmXXQTUek9VAVkzSamU00syozu9zMPgBuM7N9zOxBM1trZp9E88VJxzxlZudF89PN7Dkzuy7a920zO6GR+w4zs2fMbJOZPW5mN5nZnRnizibGn5jZ36PzPWpm+yZt/7qZrTaz9WZ2ZT3X5wgz+8DM2ietO83MXo3mx5nZC2b2qZm9b2a/MrNOGc51u5n9NGn5suiYNWZ2bsq+J5nZy2a20czeNbOrkzY/E31+amafmdmRiWubdPx4M1toZhuiz/HZXpu9vM59zOy26Gf4xMzmJ2071cwWRz/DW2Y2OdN1lvgoQUhTDQT6AEOBmYTfqdui5SHAVuBX9Rx/OLAC2Bf4OfA7M7NG7HsX8CLQF7ga+Ho935lNjF8DZgD9gU7AvwGY2UjgN9H594++r5g03P0fwGbgiynnvSua3w18N/p5jgSOBS6qJ26iGCZH8XwJGA6ktn9sBr4B9AZOAi40s69G246JPnu7e3d3fyHl3H2AvwI3Rj/bL4C/mlnflJ9hj2uTRkPX+Q5CleWo6FzXRzGMA/4AXBb9DMcAlRm+Q+Lk7po0ZT0R/qMeF81PBHYAXerZvxT4JGn5KUIVFcB0YGXStiLAgYF7sy/h5rMLKErafidwZ5Y/U7oYf5i0fBHwcDT/Y2Be0rZu0TU4LsO5fwrcGs33INy8h2bY91Lg/qRlBz4Xzd8O/DSavxX4WdJ+ByXvm+a8NwDXR/Ml0b4dkrZPB56L5r8OvJhy/AvA9Iauzd5cZ2A/oBrYJ81+/5OIV1NhJ5UgpKnWuvu2xIKZFZnZ/0RVMBsJVRq9k6tZUnyQmHH3LdFs973cd3/g46R1AO9mCjjLGD9Imt+SFNP+yed2983A+kzfRSgtnG5mnYHTgZfcfXUUx0FRtcsHURz/SShNNKRODMDqlJ/vcDN7Mqra2QDMyvK8iXOvTlm3GhiUtJzp2tTRwHUeTPg3+yTNoYOBt7KMV2KkBCFNlfoY3L8CBwOHu3tPaqs0MlUb5cL7QB8zK0paN7ie/ZsS4/vJ546+s2+mnd19GeEGewJ1q5cgVFUtJzx91BP498bEQChBJbsLeAAY7O69gJuTztvQY4trCFVCyYYA72URV6r6rvO7hH+z3mmOexc4sBHfJzmmBCG51oNQ1/xpVJ99VdxfGP1FXgFcbWadzOxI4Csxxfgn4GQzmxA1KF9Dw/+P7gIuIdwg/zcljo3AZ2Y2ArgwyxjuAaab2cgoQaXG34Pw1/m2qD7/a0nb1hKqdg7IcO4FwEFm9jUz62BmZwMjgQezjC01jrTX2d3fBx4Cfh01Znc0s0QC+R0ww8yONbN2ZjYouj6SZ0oQkms3AF2BdcA/gIfz9L3TCA296wn1/ncT+mukcwONjNHdlwLfItz03wc+AaoaOOyPhPaav7n7uqT1/0a4eW8CboliziaGh6Kf4W/Ayugz2UXANWa2idBmck/SsVuA2cDfo6enjkg593rgZMJf/+uB7wMnp8SdrRuo/zp/HdhJKEV9RGiDwd1fJDSCXw9sAJ5mz1KN5IE6ykmrZGZ3A8vdPfYSjEhrpRKEtApm9nkzOzCqkpgMnArML3BYIi2aelJLazEQuI/QYFwFXOjuLxc2JJGWTVVMIiKSlqqYREQkrVZVxbTvvvt6SUlJocMQEWkxFi1atM7d+6Xb1qoSRElJCRUVFYUOQ0SkxTCz1J7zNVTFJCIiaSlBiIhIWkoQIiKSVqtqg0hn586dVFVVsW3btoZ3lrzr0qULxcXFdOzYsdChiEiKVp8gqqqq6NGjByUlJWR+D40Ugruzfv16qqqqGDZsWKHDEZEUrb6Kadu2bfTt21fJoRkyM/r27avSnUgjzZ0LJSXQrl34nDs3t+dv9SUIQMmhGdO/jUjjzJ0LM2fClug1WatXh2WAadNy8x2tvgQhItJcNaUEcOWVtckhYcuWsD5XlCBitH79ekpLSyktLWXgwIEMGjSoZnnHjh31HltRUcEll1zS4HeMHz8+V+GKSB4lSgCrV4N7bQkg2yTxzjt7t74xlCBS5LJOr2/fvixevJjFixcza9Ysvvvd79Ysd+rUiV27dmU8try8nBtvvLHB73j++ecbH6CIFExTSwBDUl8028D6xlCCSNLUjJ6N6dOn873vfY9JkyZx+eWX8+KLLzJ+/HjGjh3L+PHjWbFiBQBPPfUUJ598MgBXX3015557LhMnTuSAAw6okzi6d+9es//EiRM588wzGTFiBNOmTSMxUu+CBQsYMWIEEyZM4JJLLqk5b7LKykqOPvpoysrKKCsrq5N4fv7znzN69GjGjBnDFVdcAcDKlSs57rjjGDNmDGVlZbz1lt4xL21PU/6gbGoJYPZsKCqqu66oKKzPGXdvNdNhhx3mqZYtW7bHukyGDnUPqaHuNHRo1qfI6KqrrvJrr73WzznnHD/ppJN8165d7u6+YcMG37lzp7u7P/bYY3766ae7u/uTTz7pJ510Us2xRx55pG/bts3Xrl3rffr08R07dri7e7du3Wr279mzp7/77ru+e/duP+KII/zZZ5/1rVu3enFxsa9atcrd3adMmVJz3mSbN2/2rVu3urv7G2+84YlruWDBAj/yyCN98+bN7u6+fv16d3cfN26c33fffe7uvnXr1prtjbE3/0YizcWdd7oXFdW9VxQVhfXZyMX95s47w/5m4TPb704GVHiGe2qbeIopW/mo0wM466yzaN++PQAbNmzgnHPO4c0338TM2LlzZ9pjTjrpJDp37kznzp3p378/H374IcXFxXX2GTduXM260tJSKisr6d69OwcccEBNP4OpU6cyZ86cPc6/c+dOLr74YhYvXkz79u154403AHj88ceZMWMGRdGfKn369GHTpk289957nHbaaUDo7CbS1tRXRZTNU0SzZ9d9Cgn2vgQwbVrunlhKR1VMSfJRpwfQrVu3mvkf/ehHTJo0iSVLlvCXv/wlY5+Azp0718y3b98+bftFun08yxdCXX/99QwYMIBXXnmFioqKmkZ0d9/jUdRszynS3BWyimjaNJgzB4YOBbPwOWdOvDf8vaUEkSQvdXopNmzYwKBBgwC4/fbbc37+ESNGsGrVKiorKwG4++67M8ax33770a5dO+644w52794NwPHHH8+tt97KlujPnI8//piePXtSXFzM/PnzAdi+fXvNdpGWoqltjrn4g3LaNKishOrq8NmckgMoQdRRiIz+/e9/nx/84AccddRRNTflXOratSu//vWvmTx5MhMmTGDAgAH06tVrj/0uuugifv/733PEEUfwxhtv1JRyJk+ezCmnnEJ5eTmlpaVcd911ANxxxx3ceOONHHrooYwfP54PPvgg57GLxKmpTxEV4g/KvMvUOJGLCZgMrABWAlek2b4PcD/wKvAi8H+StlUCrwGLqacRJXlqaiN1a7Vp0yZ3d6+urvYLL7zQf/GLXxQ4orr0bySN1ZRGWrP0jcRm+fn+5qK++2tsJQgzaw/cBJwAjASmmtnIlN3+HVjs7ocC3wB+mbJ9kruXunt5XHG2BbfccgulpaWMGjWKDRs2cMEFFxQ6JJEmUxVR/OKsYhoHrHT3Ve6+A5gHnJqyz0jgCQB3Xw6UmNmAGGNqkxId9JYtW8bcuXNrnkgSaclURRS/OBPEIODdpOWqaF2yV4DTAcxsHDAUSDy76cCjZrbIzGZm+hIzm2lmFWZWsXbt2pwFLyLNW1t4iqjQ4uwHkW6YztTnI38G/NLMFhPaG14GEs9vHuXua8ysP/CYmS1392f2OKH7HGAOQHl5uZ6/FGkjhgwJ1Urp1mcr7n4ELV2cJYgqYHDScjGwJnkHd9/o7jPcvZTQBtEPeDvatib6/IjQkD0uxlhFpACa0g9BVUTxizNBLASGm9kwM+sETAEeSN7BzHpH2wDOA55x941m1s3MekT7dAOOB5bEGKuI5FlTG5lVRRS/2BKEu+8CLgYeAV4H7nH3pWY2y8xmRbsdAiw1s+WEp52+E60fADxnZq8QHn/9q7s/HFescZo4cSKPPPJInXU33HADF110Ub3HVFRUAHDiiSfy6aef7rHP1VdfXdMnIZP58+ezbNmymuUf//jHPP7443sRvUh8cvE+g9b+FFGhxToWk7svABakrLs5af4FYHia41YBY+KMLV+mTp3KvHnz+PKXv1yzbt68eVx77bVZHb9gwYKGd8pg/vz5nHzyyYwcGZ4uvuaaaxp9LpFcy9fYZ9J46kkdszPPPJMHH3yQ7du3A2FY7TVr1jBhwgQuvPBCysvLGTVqFFdddVXa40tKSli3bh0As2fP5uCDD+a4446rGRYcQj+Hz3/+84wZM4YzzjiDLVu28Pzzz/PAAw9w2WWXUVpayltvvcX06dP505/+BMATTzzB2LFjGT16NOeee25NfCUlJVx11VWUlZUxevRoli9fvkdMGhpcciFfY59J47Wp0VwvvRQWL87tOUtL4YYbMm/v27cv48aN4+GHH+bUU09l3rx5nH322ZgZs2fPpk+fPuzevZtjjz2WV199lUMPPTTteRYtWsS8efN4+eWX2bVrF2VlZRx22GEAnH766Zx//vkA/PCHP+R3v/sd3/72tznllFM4+eSTOfPMM+uca9u2bUyfPp0nnniCgw46iG984xv85je/4dJLLwVg33335aWXXuLXv/411113Hb/97W/rHN+/f38ee+wxunTpwptvvsnUqVOpqKjgoYceYv78+fzzn/+kqKiIjz/+GIBp06ZxxRVXcNppp7Ft2zaqq6v3/kJLszR3bqgSeuedcGOfPTv7ap5cjGYq8VIJIg8S1UwQqpemTp0KwD333ENZWRljx45l6dKlddoLUj377LOcdtppFBUV0bNnT0455ZSabUuWLOHoo49m9OjRzJ07l6VLl9Ybz4oVKxg2bBgHHXQQAOeccw7PPFP7BPHpp58OwGGHHVYzyF+ynTt3cv755zN69GjOOuusmrizHRpcHfVaBzUyt35tqgRR31/6cfrqV7/K9773PV566SW2bt1KWVkZb7/9Ntdddx0LFy5kn332Yfr06RmH+k5IHXY7Yfr06cyfP58xY8Zw++2389RTT9V7Hm9guO7EsOGZhhVPHhq8urq65n0QrqHB25Smvg8B1A+huVMJIg+6d+/OxIkTOffcc2tKDxs3bqRbt2706tWLDz/8kIceeqjecxxzzDHcf//9bN26lU2bNvGXv/ylZtumTZvYb7/92LlzJ3OT/nzr0aMHmzZt2uNcI0aMoLKykpUrVwJhZNYvfOELWf88GhpcQI3MbYESRJ5MnTqVV155hSlTpgAwZswYxo4dy6hRozj33HM56qij6j2+rKyMs88+m9LSUs444wyOPvromm0/+clPOPzww/nSl77EiBEjatZPmTKFa6+9lrFjx9ZpGO7SpQu33XYbZ511FqNHj6Zdu3bMmjWLbGlocAE1MrcF1pqqAMrLyz3RfyDh9ddf55BDDilQRJIN/Ru1TIk2iNRGZrUjtCxmtijTiNkqQYi0YU0Z6kKNzK1fm2qkFpFaqSWAxFNIoEZmCdpECaI1VaO1Nvq3KZxcDHUhrVurTxBdunRh/fr1uhE1Q+7O+vXrax6TlfzSU0jSkFZfxVRcXExVVRV6mVDz1KVLF4qLixveUXIuF+9TkNat1SeIjh07MmzYsEKHIdLsaKgLaUirr2ISkfT0FJI0RAlCpAVrymOqoPcpSP1afRWTSGuVi8dUReqjEoRIC6XHVCVuShAiLZQeU5W4KUGItFAaLE/ipgQh0kLNnh0eS02mx1Qll5QgRFooPaYqcYs1QZjZZDNbYWYrzeyKNNv3MbP7zexVM3vRzP5PtseKtAZ6TFWas9gShJm1B24CTgBGAlPNbGTKbv8OLHb3Q4FvAL/ci2NFWrSmvtNZJG5xliDGASvdfZW77wDmAaem7DMSeALA3ZcDJWY2IMtjRVo0PaYqzV2cCWIQ8G7SclW0LtkrwOkAZjYOGAoUZ3ks0XEzzazCzCo0IJ+0JHpMVZq7OBOEpVmXOub2z4B9zGwx8G3gZWBXlseGle5z3L3c3cv79evXhHBF8kuPqUpzF2eCqAIGJy0XA2uSd3D3je4+w91LCW0Q/YC3szlWpKXTY6rS3MWZIBYCw81smJl1AqYADyTvYGa9o20A5wHPuPvGbI4Vaen0mKo0d7ElCHffBVwMPAK8Dtzj7kvNbJaZzYp2OwRYambLCU8sfae+Y+OKVaSx9JiqtGbWml7FWV5e7hUVFYUOQ9qI1NFUIVQRqRQgLYmZLXL38nTb1JNapJH0mKq0dkoQIo2kx1SltVOCEGkkPaYqrZ0ShEgj6TFVae2UIEQaSY+pSmunBCFtmh5TFcmsQ6EDECmU1MdUE6Opgm70IqAShLRhekxVpH5KENJm6TFVkfopQUibpcdUReqnBCFtlh5TFamfEoS0WXpMVaR+eopJ2rRp05QQRDJRCUJatKb2YxCRzFSCkBZL/RhE4qUShLRY6scgEi8lCGmx1I9BJF5KENJiqR+DSLyUIKTFUj8GkXgpQUiLpX4MIvHSU0zSoqkfg0h8Yi1BmNlkM1thZivN7Io023uZ2V/M7BUzW2pmM5K2VZrZa2a22Mwq4oxTCkf9GESar9hKEGbWHrgJ+BJQBSw0swfcfVnSbt8Clrn7V8ysH7DCzOa6+45o+yR3XxdXjFJY6scg0rzFWYIYB6x091XRDX8ecGrKPg70MDMDugMfA7tijEmaEfVjEGne4kwQg4B3k5aronXJfgUcAqwBXgO+4+7V0TYHHjWzRWY2M9OXmNlMM6sws4q1a9fmLnqJnfoxiDRvcSYIS7POU5a/DCwG9gdKgV+ZWc9o21HuXgacAHzLzI5J9yXuPsfdy929vF+/fjkJXPJD/RhEmrc4E0QVMDhpuZhQUkg2A7jPg5XA28AIAHdfE31+BNxPqLKSVkT9GESatzgTxEJguJkNM7NOwBTggZR93gGOBTCzAcDBwCoz62ZmPaL13YDjgSUxxioFoH4MIs1bbE8xufsuM7sYeARoD9zq7kvNbFa0/WbgJ8DtZvYaoUrqcndfZ2YHAPeHtms6AHe5+8NxxSqFo34MIs2Xuac2C7Rc5eXlXlGhLhMiItkys0XuXp5um4bakCZRRzeR1ktDbUijqaObSOuWVQnCzO41s5PMTCUOqaGObiKtW7Y3/N8AXwPeNLOfmdmIGGOSFkId3URat6wShLs/7u7TgDKgEnjMzJ43sxlm1jHOAKX5Ukc3kdYt6yojM+sLTAfOA14GfklIGI/FEpk0e+roJtK6ZdsGcR/wLFAEfMXdT3H3u93924RB9qQNUkc3kdYtq34QZvZFd/9bHuJpEvWDEBHZO7noB3GImfVOOuE+ZnZRLoITEZHmKdsEcb67f5pYcPdPgPNjiUhERJqFbBNEu+ilPkDN2+I6xROS5JN6QotIJtn2pH4EuMfMbia802EWoMHzWjj1hBaR+mTbSN0OuIAwNLcBjwK/dffd8Ya3d9RIvXdKSkJSSDV0KFRW5jsaESmE+hqpsypBRK8B/U00SSuhntAiUp9s+0EMN7M/mdkyM1uVmOIOTuKlntAiUp9sG6lvI5QedgGTgD8Ad8QVlOSHekKLSH2yTRBd3f0JQpvFane/GvhifGFJPqgntIjUJ9unmLZFDdVvRq8RfQ/oH19Yki965aeIZJJtCeJSwjhMlwCHAf8XOCemmEREpBlosAQRdYr7F3e/DPgMmBF7VCIiUnANliCivg6HJfekzpaZTTazFWa20syuSLO9l5n9xcxeMbOlZjYj22NFRCRe2bZBvAz82cz+F9icWOnu92U6ICp53AR8CagCFprZA+6+LGm3bwHL3P0rZtYPWGFmc4HdWRwrIiIxyjZB9AHWU/fJJQcyJghgHLDS3VcBmNk84FQg+SbvQI+odNId+JjwKO3hWRwrIiIxyvaVozPSTOc2cNgg4N2k5apoXbJfAYcAa4DXgO9EvbazOVbQYHsiEp+sShBmdhvhr/06GkgS6dosUs/xZWAxoWRyIOFd189meWwitpnATIAhbawLsAbbE5E4ZfuY64PAX6PpCaAn4Ymm+lQBg5OWiwklhWQzgPs8WAm8DYzI8lgA3H2Ou5e7e3m/fv2y/HFahyuvrE0OCVu2hPUiIk2V7WB99yYvm9kfgccbOGwhMNzMhhE61k0BvpayzzuEEWKfNbMBwMHAKuDTLI5t8zTYnojEKdtG6lTDgXrrc9x9V9Tr+hGgPXCruy81s1nR9puBnwC3m9lrhGqly919HUC6YxsZa6s1ZEj64brbWE2biMQk2zaITdRtA/gAuLyh49x9AbAgZd3NSfNrgOOzPVbqmj27bhsEaLA9EcmdbKuYesQdiOy9REP0lVeGaqUhQ0JyUAO1iORCtiWI04C/ufuGaLk3MNHd58cXmmRDg+2JSFyyfYrpqkRyAHD3T4GrYolIRESahWwTRLr9GtvALSIiLUC2CaLCzH5hZgea2QFmdj2wKM7ARESksLJNEN8GdgB3A/cAWwkD7YmISCuV7VNMmwENuS0i0oZkVYIws8eiJ5cSy/uY2SOxRSUiIgWXbRXTvtGTSwC4+yfondQiIq1atgmi2sxqBnAwsxIyjK4qIiKtQ7YJ4krgOTO7w8zuAJ4GfhBfWG2H3ucgIs1Vto3UD5tZOeG9C4uBPxOeZJIm0PscRKQ5M/eGa4rM7DzgO4T3MiwGjgBecPcv1ndcvpWXl3tFRUWhw8haSUn60ViHDoXKynxHIyJtkZktcvfydNuyrWL6DvB5YLW7TwLGAmtzFF+bpfc5iEhzlm2C2Obu2wDMrLO7Lye83EeaINN7G/Q+BxFpDrJNEFVRP4j5hPdG/5kMrwCV7M2eHd7fkEzvcxCR5iLbRurTotmrzexJoBfwcGxRtRF6n4OINGdZNVK3FC2tkVpEpNBy0UgtIiJtjBKEiIikpQQhIiJpxZogzGyyma0ws5Vmtsdw4WZ2mZktjqYlZrbbzPpE2yrN7LVomxoWRETyLLbXhppZe+Am4EtAFbDQzB5w92WJfdz9WuDaaP+vAN9194+TTjPJ3dfFFaOIiGQWZwliHLDS3Ve5+w5gHnBqPftPBf4YYzwiIrIX4kwQg4B3k5aronV7MLMiYDJwb9JqBx41s0VmNjPTl5jZTDOrMLOKtWs1+oeISK7EmSAszbpMnS6+Avw9pXrpKHcvA04AvmVmx6Q70N3nuHu5u5f369evaRGLiEiNOBNEFTA4abmYzMNzTCGlesnd10SfHwH3E6qsREQkT+JMEAuB4WY2zMw6EZLAA6k7mVkv4AuEd0wk1nUzsx6JeeB4YEmMsYqISIrYnmJy911mdjHwCNAeuNXdl5rZrGj7zdGupwGPuvvmpMMHAPebWSLGu9xdYz+JiOSRxmISEWnDNBaTiIjsNSUIERFJSwlCRETSUoIQEZG0lCBERCQtJQgREUlLCUJERNJSgmiiuXOhpATatQufc+cWOiIRkdyIrSd1WzB3LsycCVu2hOXVq8MywLRphYtLRCQXVIJogiuvrE0OCVu2hPUiIi2dEkQTvPPO3q0XEWlJlCCaYMiQvVsvItKSKEE0wezZUFRUd11RUVgvItLSKUE0wbRpMGcODB0KZuFzzhw1UItI66CnmJpo2jQlBBFpnVSCEBGRtJQgREQkLSUIERFJSwlCRETSUoIQEZG0lCBERCStWBOEmU02sxVmttLMrkiz/TIzWxxNS8xst5n1yeZYERGJV2wJwszaAzcBJwAjgalmNjJ5H3e/1t1L3b0U+AHwtLt/nM2xIiISrzhLEOOAle6+yt13APOAU+vZfyrwx0YeKyIiORZnghgEvJu0XBWt24OZFQGTgXsbcexMM6sws4q1a9fudZBz54YhMvTCHxGRuuJMEJZmnWfY9yvA393947091t3nuHu5u5f369dvrwKcOxfOPz8Mz+1e+8IfJQkRkXgTRBUwOGm5GFiTYd8p1FYv7e2xjXbllbB1a911euGPiEgQZ4JYCAw3s2Fm1omQBB5I3cnMegFfAP68t8c2VaYX+6xeDRs35vrbRERaltgShLvvAi4GHgFeB+5x96VmNsvMZiXtehrwqLtvbujYXMdY34t9Ro6EB3KekkREWg5zz9Qs0PKUl5d7RUVF1vvPnRvaHJLfK11UBN//Ptx7L7z2GpxxBvz3f8N++8UQsIhIgZnZIncvT7etTfekzvTCn6uugkWL4D//Ex58EA45JKyvri50xCIi+dOmSxDZePNNuOACePJJOProkChGjMjpV4iIFIxKEE0wfDg88QTceissWQJjxsB//Ads317oyERE4qUEkQUzmDEDli8PbRJXXw1jx8Lf/17oyERE4qMEsRf694e77oIFC0LD9oQJcOGFsGFDoSMTEck9JYhGOOGEUN30ve+FNolDDoHZs+GZZ/bseCci0lKpkbqJKirgkkvghRfCcseOUF4eShcTJsD48bDvvnkNSUQka/U1UitB5MjHH8Pzz8Nzz4Vp4ULYsSNsGzGiNmFMmAAHHBDaNURECk0JogC2bQuli+eeC43Zf/87fPJJ2DZwYG2yOOooKC2FDh0KGq6ItFH1JQjdlmLSpUttEoDQye7112tLGM89B3/6U9jWrRtMnRoen91//8LFLCKSTCWIAqqqCiWLxx+H3/8+lCL+9V/hssugZ89CRycibYE6yjVTxcVw9tlwyy2hj8VXvwo//Sl87nNw002wc2ehIxSRtkwJopk44IDQx2LhQhg1Ci6+OHzee294mZGISL4pQTQz5eXwt7/BX/8KnTrBmWeGhmz12haRfFOCaIbM4MQTYfFi+O1vobIyNHaffjqsWFHo6ESkrVCCaMY6dIBvfjOMKPvTn4bG7FGj4KKL4MMPCx2diLR2ShAtQLdu4T3ZK1eGsZ9uuSU0ZF9zDXz2WaGjE5HWSo+5tkBvvgk/+EFowB44MPSfOP740DkvMW3dmn4+07YuXaBXL+jdu3ZKXk7Md+5cyJ9cRHJNHeVameHDQye7F14IfSYuuGDvz2EWkkKXLuGmv2NHGJV29+76j+vSZc/k0bs3HHwwTJoERxwR9hGRlk8liBbOHR57DNasqb3hJ6auXfdcl1jfseOe40G5w+bN8OmnYdqwIf186vInn8Bbb4Xe4l26hAEKJ00K0+c/H57GEpHmSSWIVswsVC/l6lzdu4epuHjvjv300zDc+ZNPhulHPwrri4rCE1iTJsEXvwhlZRp3SqSliLUEYWaTgV8C7YHfuvvP0uwzEbgB6Aisc/cvROsrgU3AbmBXpgyXrC2WIJqr9evh6adDn44nn4Rly8L6Hj3gmGNqSxhjxkD79nse7x4a4D/6KDyxVd/nRx9B376h5HLkkeFz5Mj05xWRugoymquZtQfeAL4EVAELganuvixpn97A88Bkd3/HzPq7+0fRtkqg3N3XZfudShDN14cfwlNPhWTxt7+FhnaAffYJCaNv3z1v+plevrTPPuHtfgMGhM/+/eG998Jw62vXhn169oTDDw/JYvz4MN+rV15+VJEWpVBVTOOAle6+KgpiHnAqsCxpn68B97n7OwCJ5CCtz4ABYdyps88Oy++9V1sd9dRTIRkkbvgjRtRNAMmf/fplbtNwh1WrQqJ4/vnQiP+Tn4S2EbPQhyS5lDF8uN7LIVKfOEsQZxJKBudFy18HDnf3i5P2uYFQtTQK6AH80t3/EG17G/gEcOB/3H1Ohu+ZCcwEGDJkyGGrV6+O5eeRlmnjRnjxxZAsEkkj8Q7xvn3rVkn17x8SUP/+oSpMyUPagkKVINL990rNRh2Aw4Bjga7AC2b2D3d/AzjK3deYWX/gMTNb7u7P7HHCkDjmQKhiyulPIC1ez55w3HFhgtr3ciQSxvPPw4MP7nlc5851E0ZiSl5Onu/aNb8/l0g+xJkgqoDBScvFwJo0+6xz983AZjN7BhgDvOHuayBUO5nZ/YQqqz0ShMjeaNcuVDWNGgXnnRfWrV8fxrtKtH2sXVs7n1hetizMb9uW/ryDB8PEiaHhfeJEGDYsTz+QSIziTBALgeFmNgx4D5hCaHNI9mfgV2bWAegEHA5cb2bdgHbuvimaPx64JsZYpQ3r2zdMDUk8WZWaQD76CF5+GR56CO64I+w7dGjdhDF0aG5j3ro1DNy4bFkoEe3aBSUltdPQoeqwKE0XW4Jw911mdjHwCOEx11vdfamZzYq23+zur5vZw8CrQDXhUdglZnYAcL+FSuAOwF3u/nBcsYpkwyy0TfToEd7fkaq6OtywEw3vDz4Y3hQIoUSRnDAGD97z+HQ++yy8TGrZsrrTqlW17wlp3z7EtmtX3WMHDgzfm5w4EstDhuR/2JQtW2D16jBVVtb9rK4O1/TAA8M4YwceGKaBA+NpC6quDom+qio8MOEeRgM48MDQiVQC9aQWiUl1NSxZUvt479NPh17nEG5EyQmje/dQEkhNBMnPXHTsCAcdFBrUk6fhw0Pnw/ffDzfct98On8nTO+/smUD23782aQwYEAaF7NYtxJKYz7Sue/c9nybbuDH9zT8xn3gEOfnnGTw4xGAWeuO/8064bglFRbXJInn63OdCkkvX6XLnznAtEjf/qqo959esSf/Gxg4dwrkPOSQ8TZf4HDEi/GHQFJs2hX+bdNOGDbWjHNQ3ZdqnVy845ZTGxVWQfhCFoAQhzVl1Nbz2Wm0J4+mnQw/0VF26hBtSIgEcckj4bMpft7t2hZtiauJI3KDWrQvDrOyNDh1qE8bWrbXJL/nnGDo0TIlqr8Tn0KGw3357dmbcsSMklJUrQ8JInbZvr/v9Q4eG69K1a0gA770HH3yw51sYu3YNowMUF8OgQbXzieXq6lBl9/rrocS2fHnoq5OcVAcNqpswEvP77RcS3PbtIfZMSWD9+roxde8ekvOwYdCnT+3gmQ1NydcgYeDAkBQbQwlCpBnavRtefTUkix07ahNCSUlheoG7hxvQZ5+FZJE8pa5LXe7cec9E0L9/bquHqqtDkkski+Qksn17+ht/Yr53772PZefOUJWXSBrJn5s21e7Xs2e42b//ft3E1KlTuBaJJJA69e3buOtTXb1nMtm9OySrxlCCEBHJEfeQDJITxubNtdV1iWn//cNTc82dBusTEckRs3Dz339/OPbYQkcTrxaQ30REpBCUIEREJC0lCBERSUsJQkRE0lKCEBGRtJQgREQkLSUIERFJSwlCRETSalU9qc1sLdBcXym3L5D1+7ULQPE1jeJrGsXXNE2Jb6i790u3oVUliObMzCoydWdvDhRf0yi+plF8TRNXfKpiEhGRtJQgREQkLSWI/JlT6AAaoPiaRvE1jeJrmljiUxuEiIikpRKEiIikpQQhIiJpKUHkkJkNNrMnzex1M1tqZt9Js89EM9tgZouj6cd5jrHSzF6LvnuP1+9ZcKOZrTSzV82sLI+xHZx0XRab2UYzuzRln7xePzO71cw+MrMlSev6mNljZvZm9LlPhmMnm9mK6Fpekcf4rjWz5dG/3/1m1jvDsfX+LsQY39Vm9l7Sv+GJGY4t1PW7Oym2SjNbnOHYfFy/tPeUvP0OurumHE3AfkBZNN8DeAMYmbLPRODBAsZYCexbz/YTgYcAA44A/lmgONsDHxA68RTs+gHHAGXAkqR1PweuiOavAP4rQ/xvAQcAnYBXUn8XYozveKBDNP9f6eLL5nchxviuBv4ti3//gly/lO3/D/hxAa9f2ntKvn4HVYLIIXd/391fiuY3Aa8Dgwob1V47FfiDB/8AepvZfgWI41jgLXcvaM94d38G+Dhl9anA76P53wNfTXPoOGClu69y9x3AvOi42ONz90fdfVe0+A+gONffm60M1y8bBbt+CWZmwL8Af8z192arnntKXn4HlSBiYmYlwFjgn2k2H2lmr5jZQ2Y2Kr+R4cCjZrbIzGam2T4IeDdpuYrCJLkpZP6PWcjrBzDA3d+H8B8Y6J9mn+ZyHc8llAjTaeh3IU4XR1Vgt2aoHmkO1+9o4EN3fzPD9rxev5R7Sl5+B5UgYmBm3YF7gUvdfWPK5pcI1SZjgP8G5uc5vKPcvQw4AfiWmR2Tst3SHJPXZ6HNrBNwCvC/aTYX+vplqzlcxyuBXcDcDLs09LsQl98ABwKlwPuEapxUBb9+wFTqLz3k7fo1cE/JeFiadXt1DZUgcszMOhL+Iee6+32p2919o7t/Fs0vADqa2b75is/d10SfHwH3E4qhyaqAwUnLxcCa/ERX4wTgJXf/MHVDoa9f5MNEtVv0+VGafQp6Hc3sHOBkYJpHFdKpsvhdiIW7f+juu929Grglw/cW+vp1AE4H7s60T76uX4Z7Sl5+B5Ugciiqs/wd8Lq7/yLDPgOj/TCzcYR/g/V5iq+bmfVIzBMaM5ek7PYA8A0LjgA2JIqyeZTxL7dCXr8kDwDnRPPnAH9Os89CYLiZDYtKRFOi42JnZpOBy4FT3H1Lhn2y+V2IK77kNq3TMnxvwa5f5DhgubtXpduYr+tXzz0lP7+DcbbAt7UJmEAowr0KLI6mE4FZwKxon4uBpYQnCv4BjM9jfAdE3/tKFMOV0frk+Ay4ifD0w2tAeZ6vYRHhht8raV3Brh8hUb0P7CT8RfZNoC/wBPBm9Nkn2nd/YEHSsScSnjp5K3Gt8xTfSkLdc+J38ObU+DL9LuQpvjui361XCTes/ZrT9YvW3574nUvatxDXL9M9JS+/gxpqQ0RE0lIVk4iIpKUEISIiaSlBiIhIWkoQIiKSlhKEiIikpQQh0gAz2211R5nN2ciiZlaSPJKoSHPSodABiLQAW929tNBBiOSbShAijRS9D+C/zOzFaPpctH6omT0RDUb3hJkNidYPsPB+hleiaXx0qvZmdks03v+jZtY12v8SM1sWnWdegX5MacOUIEQa1jWliunspG0b3X0c8CvghmjdrwhDph9KGCjvxmj9jcDTHgYaLCP0wAUYDtzk7qOAT4EzovVXAGOj88yK50cTyUw9qUUaYGafuXv3NOsrgS+6+6poQLUP3L2vma0jDB+xM1r/vrvva2ZrgWJ33550jhLgMXcfHi1fDnR095+a2cPAZ4QRa+d7NEihSL6oBCHSNJ5hPtM+6WxPmt9NbdvgSYRxsQ4DFkUjjIrkjRKESNOcnfT5QjT/PGHkTIBpwHPR/BPAhQBm1t7MemY6qZm1Awa7+5PA94HewB6lGJE46S8SkYZ1tbovrn/Y3ROPunY2s38S/tiaGq27BLjVzC4D1gIzovXfAeaY2TcJJYULCSOJptMeuNPMehFG2L3e3T/N0c8jkhW1QYg0UtQGUe7u6wodi0gcVMUkIiJpqQQhIiJpqQQhIiJpKUGIiEhaShAiIpKWEoSIiKSlBCEiImn9f/zc5ravN6SyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = model.evaluate(test_tfidf, np.array(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.0482879e-01]\n",
      " [2.4172455e-02]\n",
      " [1.6421080e-05]\n",
      " [3.5166740e-06]\n",
      " [5.9604645e-08]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fcfc6106190>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the query\n",
    "test.set_index(['qid', 'sentid'], inplace=True)\n",
    "test.groupby(level=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13420"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_main = dict()\n",
    "new_list = list(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in test.index.get_level_values('qid').unique():\n",
    "    rowCounter = 0\n",
    "    rowCounter += test.loc[result,\"question\"].count()\n",
    "    dict_main[result] = new_list[:rowCounter]\n",
    "    del new_list[:rowCounter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>sentence text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th>sentid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">6</th>\n",
       "      <th>0</th>\n",
       "      <td>Which miRNAs could be used as potential biomar...</td>\n",
       "      <td>Finally, five promising differentially miRNAs ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which miRNAs could be used as potential biomar...</td>\n",
       "      <td>MiR-200a, miR-200b, miR-200c, and miR-141, all...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which miRNAs could be used as potential biomar...</td>\n",
       "      <td>Upregulation of microRNA-203 is associated wit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which miRNAs could be used as potential biomar...</td>\n",
       "      <td>multivariate analysis showed that the status o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which miRNAs could be used as potential biomar...</td>\n",
       "      <td>These findings provide the convincing evidence...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     question  \\\n",
       "qid sentid                                                      \n",
       "6   0       Which miRNAs could be used as potential biomar...   \n",
       "    1       Which miRNAs could be used as potential biomar...   \n",
       "    2       Which miRNAs could be used as potential biomar...   \n",
       "    3       Which miRNAs could be used as potential biomar...   \n",
       "    4       Which miRNAs could be used as potential biomar...   \n",
       "\n",
       "                                                sentence text  label  \n",
       "qid sentid                                                            \n",
       "6   0       Finally, five promising differentially miRNAs ...      1  \n",
       "    1       MiR-200a, miR-200b, miR-200c, and miR-141, all...      0  \n",
       "    2       Upregulation of microRNA-203 is associated wit...      0  \n",
       "    3       multivariate analysis showed that the status o...      0  \n",
       "    4       These findings provide the convincing evidence...      0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_summariser(csvfile, questionids, n):\n",
    "    temp_list = []\n",
    "    Main_result = []\n",
    "    \n",
    "    for i in questionids:\n",
    "        counter = 0\n",
    "        result = dict()\n",
    "        temp_list = dict_main[i]\n",
    "        for j in range(len(temp_list)):\n",
    "            if counter != len(temp_list):\n",
    "                result[counter] = temp_list[counter][0]\n",
    "                sorted_result = sorted(result.items(), key=lambda kv: kv[1], reverse=True)\n",
    "                store = [x for x, r in sorted_result]\n",
    "                counter+=1\n",
    "        Main_result.append(store[0:n])\n",
    "    return Main_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 25, 0, 13, 19], [9, 4, 0, 6, 5]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_summariser(test, [6,7], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0NeK3gM0dC9"
   },
   "source": [
    "# Task 4 (5 marks): Recurrent NN\n",
    "\n",
    "Use the files `training.csv`, `dev_test.csv`, and `test.csv` provided by us.\n",
    "\n",
    "Implement a more complex neural network that is composed of the following layers:\n",
    "\n",
    "* An embedding layer that generates embedding vectors of the sentence text with 35 dimensions.\n",
    "* A LSTM layer. You need to determine the size of this LSTM layer, and the text length limit (if needed).\n",
    "* The final output layer with one cell for binary classification, as in task 3.\n",
    "\n",
    "Train the model with the training data, use the dev_test set to determine a good size of the LSTM layer and an appropriate length limit (if needed), and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the LSTM layer.\n",
    "\n",
    "Based on your experiments, comment on whether this system is better than the systems developed in the previous tasks.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
    "* **1 mark** if the code passes the sentence text to the model correctly. The documentation needs to explain what decisions had to be made to process long sentences. In particular, did you need to truncate the input text, and how did you determine the length limit?\n",
    "* **1 mark** if the code returns the IDs of the *n* sentences that have the highest prediction score in the given question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the LSTM layer. The explanations must be clear and concise. To make this task less time-consuming, use $n=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "c8RRCWeQTrPl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense, LSTM, Flatten\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37951"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab_size = len(train_text)\n",
    "encoded_train = [one_hot(d,Vocab_size) for d in train_text]\n",
    "encoded_dev_test = [one_hot(d,Vocab_size) for d in dev_test_text]\n",
    "encoded_test = [one_hot(d,Vocab_size) for d in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14267,\n",
       " 7928,\n",
       " 31827,\n",
       " 7158,\n",
       " 32620,\n",
       " 16248,\n",
       " 22844,\n",
       " 16495,\n",
       " 6686,\n",
       " 7928,\n",
       " 31827,\n",
       " 7158,\n",
       " 18725,\n",
       " 31587,\n",
       " 106,\n",
       " 10992]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382\n"
     ]
    }
   ],
   "source": [
    "trainMax = [] \n",
    "for i in range(len(encoded_train)):\n",
    "    trainMax.append(len(encoded_train[i]))\n",
    "print(max(trainMax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "dev_testMax = [] \n",
    "for j in range(len(encoded_dev_test)):\n",
    "        dev_testMax.append(len(encoded_dev_test[j]))\n",
    "print(max(dev_testMax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401\n"
     ]
    }
   ],
   "source": [
    "testMax = [] \n",
    "for q in range(len(encoded_test)):\n",
    "            testMax.append(len(encoded_test[q]))\n",
    "print(max(testMax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train = pad_sequences(encoded_train, maxlen=max(trainMax))\n",
    "padded_dev_test = pad_sequences(encoded_dev_test, maxlen=max(trainMax))\n",
    "padded_test = pad_sequences(encoded_test, maxlen=max(trainMax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 382, 35)           1328285   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 35)                9940      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 36        \n",
      "=================================================================\n",
      "Total params: 1,338,261\n",
      "Trainable params: 1,338,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(len(train_text), 35, input_length = max(count)))\n",
    "model.add(layers.LSTM(35)) \n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37951 samples, validate on 12741 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 23:55:12.412697: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_7804_7986' and '__inference___backward_standard_lstm_8137_8622_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_8806' both implement 'lstm_b708ed23-b126-43c9-aed6-4b46b85c095d' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37000/37951 [============================>.] - ETA: 2s - loss: 0.6193 - accuracy: 0.6936"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 23:56:52.860054: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_9120_specialized_for_sequential_2_lstm_1_StatefulPartitionedCall_at___inference_distributed_function_9479' and '__inference_cudnn_lstm_with_fallback_9231' both implement 'lstm_bbb23594-c1f8-46b8-ae04-a456a590bd55' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37951/37951 [==============================] - 116s 3ms/sample - loss: 0.6188 - accuracy: 0.6938 - val_loss: 0.6076 - val_accuracy: 0.6973\n",
      "Epoch 2/10\n",
      "37951/37951 [==============================] - 124s 3ms/sample - loss: 0.5741 - accuracy: 0.7016 - val_loss: 0.6142 - val_accuracy: 0.6810\n",
      "Epoch 3/10\n",
      "31000/37951 [=======================>......] - ETA: 20s - loss: 0.5339 - accuracy: 0.7409"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(padded_train, np.array(train_label), \n",
    "                    epochs = 10, \n",
    "                    batch_size = 1000, \n",
    "                    validation_data = (padded_dev_test, np.array(dev_test_label)))\n",
    "                    #callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppkBsuB_0dC9"
   },
   "source": [
    "# Submission of results\n",
    "\n",
    "Your submission should consist of this Jupyter notebook with all your code and explanations inserted into the notebook as text cells. **The notebook should contain the output of the runs. All code should run. Code with syntax errors or code without output will not be assessed.**\n",
    "\n",
    "**Do not submit multiple files. If you feel you need to submit multiple files, please contact Diego.Molla-Aliod@mq.edu.au first.**\n",
    "\n",
    "Examine the text cells of this notebook so that you can have an idea of how to format text for good visual impact. You can also read this useful [guide to the MarkDown notation](https://daringfireball.net/projects/markdown/syntax),  which explains the format of the text cells.\n",
    "\n",
    "Each task specifies a number of marks. The final mark of the assignment is the sum of all the marks of each individual task.\n",
    "\n",
    "By submitting this assignment you are acknowledging that this is your own work. Any submissions that break the code of academic honesty will be penalised as per [the academic integrity policy](https://policies.mq.edu.au/document/view.php?id=3).\n",
    "\n",
    "Late submissions **will not be accepted** without an approved [Special Consideration](http://from.mq.edu.au/MT0X0E0FUrrU200rm0JB0U0) request.  Assessments submitted after the due date will receive a mark of **zero**."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "A2_solution.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a7b63e7410c98f344f02082f10d790581d1dba1eeb1c8fe30f342f6109f0429e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
